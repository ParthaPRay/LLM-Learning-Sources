This repo contains a list of sources, weblinks, blogs and Youtube channels from where LLMs can and should be learned.





# Large Language Model

* How Large Language Models Work, 
https://www.youtube.com/watch?v=5sLYAQS9sWQ&ab_channel=IBMTechnology

* **Andrej Karpathy**

    * 1hr Talk Intro to Large Language Models Lecture by **Andrej Karpathy**, https://www.youtube.com/watch?v=zjkBMFhNj_g&ab_channel=AndrejKarpathy
    
      Slide PDF: https://drive.google.com/file/d/1pxx_ZI7O-Nwl7ZLNk5hI3WzAsTLwvNU7/view
    
      Slide PPT Keynote: https://drive.google.com/file/d/1FPUpFMiCkMRKPFjhi9MAhby68MHVqe8u/view


              Makemore implementation from Andrej Karpathy
      
              https://github.com/karpathy/makemore
   
    
    * **Neural Networks: Zero to Hero** Lecture by **Andrej Karpathy**

      A course on neural networks that starts all the way at the basics. The course is a series of YouTube videos where we code and train neural networks together. The Jupyter notebooks we build in the videos are then captured here inside the lectures directory [https://github.com/karpathy/nn-zero-to-hero/blob/master/lectures]. Every lecture also has a set of exercises included in the video description. (This may grow into something more respectable).
      
      https://github.com/karpathy/nn-zero-to-hero/tree/master
      
    * Let's build GPT: from scratch, in code, spelled out.,
          
        ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/d4894e83-ddb2-46fa-bcab-7dba61aeaac6)
      
       https://www.youtube.com/watch?v=kCc8FmEb1nY&ab_channel=AndrejKarpathy

       https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing
    

     * Let's build the GPT Tokenizer, 
        https://www.youtube.com/watch?v=zduSFxRajkE&ab_channel=AndrejKarpathy

       https://colab.research.google.com/drive/1y0KnCFZvGVf_odSfcNAws6kcDD7HsI0L?usp=sharing

       https://github.com/karpathy/minbpe
       
        
* Create a Large Language Model from Scratch with Python ‚Äì Tutorial, https://www.youtube.com/watch?v=UU1WVnMk4E8&t=24s&ab_channel=freeCodeCamp.org

  
* How to Build an LLM from Scratch | An Overview,
 https://www.youtube.com/watch?v=ZLbVdvOoTKM&pp=ygUdQ3JlYXRlIGEgTGFyZ2UgTGFuZ3VhZ2UgTW9kZWw%3D
  
* Train your own language model with nanoGPT | Let‚Äôs build a songwriter, https://www.youtube.com/watch?v=XS8eRtlcCGU&ab_channel=SophiaYang

* A Hackers' Guide to Language Models, https://www.youtube.com/watch?v=jkrNMKz9pWU&ab_channel=JeremyHoward

* Create your own Local Chatgpt for FREE, Full Guide: PDF, Image, & Audiochat (Langchain, Streamlit), https://www.youtube.com/watch?v=CUjO8b6_ZuM&t=452s&ab_channel=LeonExplainsAI
  
* LLM Evaluation Essentials: Statistical Analysis of Hallucination LLM Evaluations, https://www.youtube.com/watch?v=IH45ltIMC3k&ab_channel=ArizeAI

  https://docs.arize.com/phoenix/llm-evals/running-pre-tested-evals/hallucinations

* Fine Tuning and Evaluating LLMs with Anyscale and Arize, https://www.youtube.com/watch?v=b-MfkFz-A2E&ab_channel=ArizeAI

* Advanced LLM Evaluation: Synthetic Data Generation, https://www.youtube.com/watch?v=AYehm7q6Oks&ab_channel=ArizeAI

* Constructing an Evaluation Approach for Generative AI Models with Hugging Face's Rajiv Shah, https://www.youtube.com/watch?v=PtXOQDHPddE&ab_channel=ArizeAI

* LLM Evaluation Essentials: Benchmarking and Analyzing Retrieval Approaches, https://www.youtube.com/watch?v=ExO3U0M3y_0&ab_channel=ArizeAI

* Building And Troubleshooting An Advanced LLM Query Engine, https://www.youtube.com/watch?v=_zDDErOaUqc&ab_channel=ArizeAI

* Model Monitoring for LLMs, 
https://www.youtube.com/watch?v=zR1X5R_1TUw&ab_channel=SethJuarez

* Let's pretrain a 3B LLM from scratch: on 16+ H100 GPUs, no detail skipped.
  https://youtu.be/aPzbR1s1O_8?si=2VEoUt9FFRUftctv

* A simple generative ML model with just KNN, https://www.youtube.com/watch?v=aFuHPiJu0QA

* Easily Train a Specialized LLM: PEFT, LoRA, QLoRA, LLaMA-Adapter, and More, https://cameronrwolfe.substack.com/p/easily-train-a-specialized-llm-peft#:~:text=LoRA%3A%20Low%2DRank%20Adaptation%20of%20Large%20Language%20Models%20%5B1%5D&text=LoRA%20leaves%20the%20pretrained%20layers,of%20the%20model%3B%20see%20below.

* The N Implementation Details of RLHF with PPO,
  https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo

* Optimizing your LLM in production

  https://huggingface.co/blog/optimize-llm

* LLM Tutorial, https://www.youtube.com/watch?v=JvLiEdTKKgk&list=PLpqh-PUKX-i4TT-vZXhFwI8Jdqr7J742n&pp=iAQB

* Serve a custom LLM for over 100 customers

  https://youtu.be/1TU9ZrZhqw0?si=LwtZJ0V2K6xQvSBA


      **Mixture of Experts (MoEs)**
      
        * Mixture of Experts Explained
      
          https://huggingface.co/blog/moe

        * Welcome Mixtral - a SOTA Mixture of Experts on Hugging Face

          https://huggingface.co/blog/mixtral

        * SegMoE: Segmind Diffusion Mixture of Experts (MoEs) Model,  https://www.youtube.com/watch?v=gIz7Td6WfEo
      
        * Mixtral Fine tuning and Inference, https://www.youtube.com/watch?v=EXFbZfp8xCI&ab_channel=TrelisResearch
       
        * Understanding Mixture of Experts, https://www.youtube.com/watch?v=0U_65fLoTq0&ab_channel=TrelisResearch
      
        * How To Install Uncensored Mixtral Locally For FREE! (EASY), https://www.youtube.com/watch?v=DC2te4CZXeM&ab_channel=WorldofAI
      
        * Fully Uncensored MIXTRAL Is Here üö® Use With EXTREME Caution, https://www.youtube.com/watch?v=q2KpPUOsBCs&ab_channel=MatthewBerman
      
        * Depliy your AI Streamlit App, https://youtu.be/74c3KaAXPvk?si=mHuW18-fvW1sJswn
          
        * makeMoE: Implement a Sparse Mixture of Experts Language Model from Scratch
      
          https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch
      
          https://github.com/AviSoori1x/makeMoE/tree/main
      






# Transformers

* What are Transformers and GPTs?, https://www.youtube.com/watch?v=ucityipiNtA&ab_channel=RicardoCalix

* High overview of the original Transformer architecture for Large Language Models, https://www.youtube.com/watch?v=zxVhAYkSYcY&ab_channel=RicardoCalix

*  Coding a Transformer from scratch on Pytorch with full explanation training and Inference, https://youtu.be/ISNdQcPhsts?si=EA3BSRVo1Tr4Z4NC
  
    * GPTs, BERTs, Full Transformers, in PyTorch (Part 1), https://www.youtube.com/watch?v=s6gys0iozLk&ab_channel=RicardoCalix
    * GPTs, BERTs, Full Transformers, in PyTorch (Part 2), https://www.youtube.com/watch?v=a1qomZy_yfo&ab_channel=RicardoCalix
    * GPU Scholar cloud, GPTs, BERTs, Full Transformers, in PyTorch (Part 3), https://www.youtube.com/watch?v=klQnQMoy9zI&ab_channel=RicardoCalix
    * Embeddings, GPTs, BERTs, Full Transformers, in PyTorch (Part 4), https://www.youtube.com/watch?v=yNZCcF6a7a4&ab_channel=RicardoCalix
    * The simple linear algebra for Attention, GPTs, BERTs, and Full Transformers in PyTorch (part 5), https://www.youtube.com/watch?v=zgH69JoAB_k&ab_channel=RicardoCalix

* Implementing a simple GPT in PyTorch, https://www.youtube.com/watch?v=RsQxg913eXY&ab_channel=RicardoCalix
* Implementing a simple GPT in PyTorch (Take Two), https://www.youtube.com/watch?v=zyDzpVu9lyA&ab_channel=RicardoCalix


* Starting with GPTs (A Hello World Example), https://www.youtube.com/watch?v=oPcJg3QrKf4&ab_channel=RicardoCalix
  
* Intro to Reinforcement Learning through Human Feedbacks (RLHF), https://www.youtube.com/watch?v=A8YqZKGRTAM&ab_channel=RicardoCalix

* What is an instruct model? - Instruction and Chat Fine-Tuning,

  As you browse the ever growing global catalogue of generative AI models, you will see some of the Large Language Models (LLMs) being listed with the suffix 'instruct' or 'chat'. What does this mean?
  
  TL:DR; The 'instruct' version of the model has been fine-tuned to be able to follow prompted instructions. These models 'expect' to be asked to do something. Models with the 'chat' suffix have been fine-tuned to work in chatbots. These models 'expect' to be involved in a conversation with different actors. In contrast non-instruct tuned models will simply generate an output that follows on from the prompt. If you are making a chatbot, implementing RAG or using agents, use instruct or chat models. If in doubt us an instruct model.

https://community.aws/content/2ZVa61RxToXUFzcuY8Hbut6L150/what-is






  

# Ollama

  * Ollama, https://github.com/ollama/ollama
    
  * Installing Ollama to Customize My Own LLM,    https://www.youtube.com/watch?v=xa8pTD16SnM&ab_channel=Decoder
  
  * Use Your Self-Hosted LLM Anywhere with Ollama Web UI,   https://www.youtube.com/watch?v=syR0fT0rkgY&ab_channel=Decoder

  * Importing Open Source Models to Ollama, https://www.youtube.com/watch?v=fnvZJU5Fj3Q&ab_channel=Decoder
 
  * Ollama has a Python library!, https://www.youtube.com/watch?v=JwYwPiOh72w&ab_channel=LearnDatawithMark
    
  * Building a local ChatGPT with Chainlit, Mixtral, and Ollama, https://www.youtube.com/watch?v=MiJQ_zlnBeo&ab_channel=LearnDatawithMark
    
  * Langroid: Chat to a CSV file using Mixtral (via Ollama), https://www.youtube.com/watch?v=XFTFEKYLxyU
    
  * Few Shot Prompting with Llama2 and Ollama, https://www.youtube.com/watch?v=ocfzGBnhhDE
 
  * Hugging Face GGUF Models locally with Ollama, https://www.youtube.com/watch?v=7BH4C6-HP14&ab_channel=LearnDatawithMark
 
  * Autogen: Ollama integration ü§Ø Step by Step Tutorial. Mind-blowing!, https://www.youtube.com/watch?v=UQw04VW60U0&ab_channel=MervinPraison
    
  * Writing Better Code with Ollama, https://www.youtube.com/watch?v=NNBWmIve3fQ&ab_channel=MattWilliams
 
  * Ollama meets LangChain, https://www.youtube.com/watch?v=k_1pOF1mj8k&ab_channel=SamWitteveen
    
  * Running Mixtral on your machine with Ollama, https://www.youtube.com/watch?v=rfr4p0srlqs&ab_channel=LearnDatawithMark
    
  * Running Mistral AI on your machine with Ollama, https://www.youtube.com/watch?v=NFgEgqua-fg&ab_channel=LearnDatawithMark
    
  * Ollama Python Library Released! How to implement Ollama RAG? https://www.youtube.com/watch?v=4HfSfFvLn9Q&ab_channel=MervinPraison

  * Ollama Web UI ü§Ø How to run LLMs 100% LOCAL in EASY web interface? CRAZY!!üöÄ (Step-by-Step Tutorial), https://www.youtube.com/watch?v=84vGNkW1A8s&ab_channel=MervinPraison
 
  * How TO Install Ollama Web UI | ChatGPT LIKE Interface, https://www.youtube.com/watch?v=bt4AR7sK9tk&ab_channel=DataScienceBasics
 
  * Ollama: The Easiest Way to Run Uncensored Llama 2 on a Mac, https://www.youtube.com/watch?v=tIRx-Sm3xDQ&ab_channel=IanWootten
 
  * Using Ollama To Build a FULLY LOCAL "ChatGPT Clone", https://www.youtube.com/watch?v=rIRkxZSn-A8&ab_channel=MatthewBerman

  * Hugging Face GGUF Models locally with Ollama, https://www.youtube.com/watch?v=7BH4C6-HP14&t=8s&ab_channel=LearnDatawithMark
 
  * Using the Chat Endpoint in the Ollama API, https://www.youtube.com/watch?v=QUJHEvCqhdw&ab_channel=MattWilliams
 
  * Adding Custom Models to Ollama, https://www.youtube.com/watch?v=0ou51l-MLCo&t=211s&ab_channel=MattWilliams
 
  * Finally Ollama has an OpenAI compatible API, https://www.youtube.com/watch?v=38jlvmBdBrU&ab_channel=MattWilliams

  * Hosting Ollama Starts With Environment Variables, https://www.youtube.com/watch?v=H_cqBjDVinw&ab_channel=MattWilliams

  * Understanding How Ollama Stores Models, https://www.youtube.com/watch?v=6bF1uCHTFyk&ab_channel=MattWilliams
    
  * Run any AI model remotely for free on google colab, https://www.youtube.com/watch?v=Qa1h7ygwQq8&ab_channel=TechwithMarco

    https://github.com/marcogreiveldinger/videos/tree/main/ollama-ai/run-on-colab

  * Run Mixtral 8x7B MoE in Google Colab, https://www.youtube.com/watch?v=Zo3CTapKJ4I&ab_channel=PromptEngineering

    https://github.com/dvmazur/mixtral-offloading?tab=readme-ov-file

    https://huggingface.co/lavawolfiee/Mixtral-8x7B-Instruct-v0.1-offloading-demo

  * Run Mixtral 8x7B Hands On Google Colab for FREE | End to End GenAI Hands-on Project
 
    https://www.youtube.com/watch?v=vzUJ-yjA8Bw&ab_channel=AnalyticsVidhya

    https://drive.google.com/drive/folders/1Bo4sJu9vEnjzV_h4FmBNb6dSZ8BxZxpa

    https://drive.google.com/drive/folders/1AuReI63WzKRSdzRIlCxl6WuBkNMryPv9

  * Unleash the power of Local LLM's with Ollama x AnythingLLM, https://www.youtube.com/watch?v=IJYC6zf86lU&ab_channel=TimCarambat

    Any LLM, unlimited documents, and fully private. All on your desktop. https://useanything.com/download

  * Ollama: How To Create Custom Models From HuggingFace ( GGUF ), https://www.youtube.com/watch?v=TFwYvHZV6j0&t=72s&ab_channel=DataScienceBasics
  






# Fine Tuning




  * Fine-tuning Llama 2 on Your Own Dataset | Train an LLM for Your Use Case with QLoRA on a Single GPU, https://www.youtube.com/watch?v=MDA3LUKNl1E&ab_channel=VenelinValkov

    https://github.com/curiousily/Get-Things-Done-with-Prompt-Engineering-and-LangChain
    
    
  * Fine-tuning Tiny LLM on Your Data | Sentiment Analysis with TinyLlama and LoRA on a Single GPU, https://www.youtube.com/watch?v=_KPEoCSKHcU&ab_channel=VenelinValkov
    
  * Make LLM Fine Tuning 5x Faster with Unsloth, https://www.youtube.com/watch?v=sIFokbuATX4&ab_channel=AIAnytime
  
  * Fine-Tuning Your Own Llama 2 Model, https://www.youtube.com/watch?v=Pb_RGAl75VE&ab_channel=DataCamp
  
  * Fine Tune a Multimodal LLM "IDEFICS 9B" for Visual Question Answering, https://www.youtube.com/watch?v=usoTCfyQxjU&ab_channel=AIAnytime
  
  * Anyone can Fine Tune LLMs using LLaMA Factory: End-to-End Tutorial, https://www.youtube.com/watch?v=iMD7ba1hHgw&t=15s&ab_channel=AIAnytime
  
  * Fine Tune Phi-2 Model on Your Dataset, https://www.youtube.com/watch?v=eLy74j0KCrY&ab_channel=AIAnytime
  
  * LLM Fine Tuning Crash Course: 1 Hour End-to-End Guide, https://www.youtube.com/watch?v=mrKuDK9dGlg
  
  * Fine-tuning LLMs with PEFT and LoRA, https://www.youtube.com/watch?v=Us5ZFp16PaU&ab_channel=SamWitteveen

  * Train a Small Language Model for Disease Symptoms | Step-by-Step Tutorial, https://www.youtube.com/watch?v=1ILVm4IeNY8&ab_channel=AIAnytime
 
  * Fine tuning Whisper for Speech Transcription, https://www.youtube.com/watch?v=anplUNnkM68&ab_channel=TrelisResearch
 
  * Efficient Fine-Tuning for Llama-v2-7b on a Single GPU, https://www.youtube.com/watch?v=g68qlo9Izf0&t=17s&ab_channel=DeepLearningAI

  * Efficient Fine-Tuning for Llama 2 on Custom Dataset with QLoRA on a Single GPU in Google Colab, https://www.youtube.com/watch?v=YyZqcNo4hdo&pp=ygUQZmluZSB0dW5pbmcgTExNXA%3D%3D
    
  * Direct Preference Optimization (DPO), https://www.youtube.com/watch?v=E5kzAbD8D0w&ab_channel=TrelisResearch
 
  * Fine-tuning Language Models for Structured Responses with QLoRa, https://www.youtube.com/watch?v=OQdp-OeG1as&ab_channel=TrelisResearch
 
  * Fine Tune LLaMA 2 In FIVE MINUTES! - "Perform 10x Better For My Use Case", https://www.youtube.com/watch?v=74NSDMvYZ9Y&ab_channel=MatthewBerman
 
  * How to Fine-Tune Mistral 7B on Your Own Data, https://www.youtube.com/watch?v=kmkcNVvEz-k&ab_channel=brev
    
  * Fine-Tune Your Own Tiny-Llama on Custom Dataset, https://www.youtube.com/watch?v=OVqe6GTrDFM&ab_channel=PromptEngineering
 
  * Fine-tune Mixtral 8x7B (MoE) on Custom Data - Step by Step Guide, https://www.youtube.com/watch?v=RzSDdosu_y8&ab_channel=PromptEngineering
    
  * Mistral: Easiest Way to Fine-Tune on Custom Data, https://www.youtube.com/watch?v=lCZRwrRvrWg&ab_channel=PromptEngineering
    
  * Self-Play Fine-Tuning (SPIN), https://www.youtube.com/watch?v=khPq69GgPAo&ab_channel=FahdMirza

    The official implementation of Self-Play Fine-Tuning (SPIN), https://github.com/uclaml/SPIN
    
    https://uclaml.github.io/SPIN/

    
  * Building Production-Ready RAG Applications: Jerry Liu, https://www.youtube.com/watch?v=TRjq7t2Ms5I&t=10s&ab_channel=AIEngineer

  * Custom Fine-tuning 30x Faster on T4 GPUs with UnSloth AI, https://www.youtube.com/watch?v=R4CUKAHShyE&ab_channel=PromptEngineering

    https://unsloth.ai/introducing

  *  To Fine Tune or not Fine Tune? That is the question, https://www.youtube.com/watch?v=XPU8PH0_d6g&ab_channel=SethJuarez

  * Fine-tune TinyLlama 1.1B locally on own custom dataset, https://youtu.be/VoDHpnCN6PA?si=Aq7soXO6k83mJJVs
    
  * Llama Factory: How to Fine-Tune LLMs easily?, https://youtu.be/G5ENOwfPHFE?si=2BZ6Zh5x55TDr2dl
    
  * How to create custom datasets to train Llama-2? https://youtu.be/z2QE12p3kMM?si=j52ptrx0GMnj9OSy
  
  * LocalGPT: Convert your chats with Docs to Fine-Tuing datasets, https://youtu.be/2_o6epQToVY?si=CZMdu1u2IU0wXUz8
    
  * D2SLM (Doc to Dataset to Fine-Tune Small Language Model), https://www.youtube.com/watch?v=khIDeJwBf4k&ab_channel=AIMakerspace
    
  * LLAMA-2 ü¶ô: EASIET WAY To FINE-TUNE ON YOUR DATA üôå, https://www.youtube.com/watch?v=LslC2nKEEGU&t=2s&ab_channel=PromptEngineering
 
  * The EASIEST way to finetune LLAMA-v2 on local machine!, https://www.youtube.com/watch?v=3fsn19OI_C8&ab_channel=AbhishekThakur
    
  * Stable Diffusion XL (SDXL) DreamBooth: Easy, Fast & Free | Beginner Friendly, https://www.youtube.com/watch?v=3fsn19OI_C8&ab_channel=AbhishekThakur
    
  * Fine-tuning Notebook on how to fine-tune MPT-7B on a free Google Colab instance to turn the model into a Chatbot. MPT7b sharded version + LoRA adapter  

    https://colab.research.google.com/drive/1HCpQkLL7UXW8xJUJJ29X7QAeNJKO0frZ?usp=sharing

    Dataset: https://huggingface.co/datasets/timdettmers/openassistant-guanaco

    
  * Fine tuning Google Colab notebook - This notebook shows how to fine-tune a 4bit model on a downstream task using the Hugging Face ecosystem. We show that it is possible to fine tune _GPT-neo-X 20B_ on a Google Colab instance!

     https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing
     
  * Fine Tune pre-trained GPT and BERT models with the Huggingface library, https://www.youtube.com/watch?v=g1dAsgibRcw&ab_channel=RicardoCalix
 
  * Fine-Tuning HF examples on GPU Scholar, scratch disk space, https://www.youtube.com/watch?v=_S01y-JY8k4&ab_channel=RicardoCalix

  * Fine-tune Multi-modal Vision and Language Models, https://www.youtube.com/watch?v=eIziN2QUt8U&ab_channel=TrelisResearch
 

  * Fine-tuning a Code LLM on Custom Code on a single GPU

    https://github.com/huggingface/cookbook/tree/main/notebooks/en

  * Fine-tuning a large language model on Kaggle Notebooks (or even on your own computer) for solving real-world tasks

    https://huggingface.co/blog/lmassaron/fine-tuning-llms-on-kaggle-notebooks

         Code references:

         Fine-tune Llama-2 for Sentiment Analysis: https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis
         Fine-tune Mistral v0.2 for Sentiment Analysis: https://www.kaggle.com/code/lucamassaron/fine-tune-mistral-v0-2-for-sentiment-analysis
         Fine-tune Phi 2 for Sentiment Analysis: https://www.kaggle.com/code/lucamassaron/fine-tune-phi-2-for-sentiment-analysis
         LSTM Baseline for Sentiment Analysis): https://www.kaggle.com/code/lucamassaron/lstm-baseline-for-sentiment-analysis



  * Phinetuning 2.0

    Meet Phi-2, Microsoft‚Äôs newly released small model, remarkably powerful yet compact. This tutorial will guide you through fine-tuning Phi-2, demonstrating how to build a unique dataset and fine-tune the model using QLoRA.

    https://huggingface.co/blog/g-ronimo/phinetuning



  * Preference Tuning LLMs with **Direct Preference Optimization** Methods

    https://huggingface.co/blog/pref-tuning

  * Fine-tune Llama 2 with DPO
    
    https://huggingface.co/blog/dpo-trl


  * Fine-tuning Llama 2 70B using PyTorch FSDP
    
    https://huggingface.co/blog/ram-efficient-pytorch-fsdp


 * Fine-Tune W2V2-Bert for low-resource ASR with ü§ó Transformers

   https://huggingface.co/blog/fine-tune-w2v2-bert

 * Google Gemma Finetuning: how to teach a large language model?,
    https://youtu.be/RevZAM9taFk?si=QuNJAVrLdqs7SUgE

 * Steps to Master Fine Tuning LLMs To Ultimate AI Proficiency : A Definitive Guide

   https://www.youtube.com/watch?v=GK860luUyEk&ab_channel=KamalrajMM

* Fine tuing optimization DoRA, NEFT, LoRA+, Unsloth
  
https://youtu.be/ae2lbmtTY5A?si=0NXaw8tOXqh800x2

  supervised fine tuning 
  https://huggingface.co/docs/trl/main/en/sft_trainer

* Fine Tune Large Language Model (LLM) on a Custom Dataset with QLoRA

  https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07

* Unsloth: How to Train LLM 5x Faster and with Less Memory Usage?

   ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/6f6e4936-84d7-46df-a573-073bf3f54494)

     https://www.youtube.com/watch?v=Gpyukc6c0w8&t=16s&ab_channel=MervinPraison


* Fine-tuning large language models (LLMs) in 2024

   Life Cycle of LLM
    ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/ed9f1bcc-f61b-4d93-b46f-e2c299cf13a1)

   Fine Tuning
  
   ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/dd0d55d8-31ee-4e9f-9412-20c82a206b18)

    Supervised fine-tuning (SFT)
    ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/be10b8d8-4a45-4bf9-b816-1a0a4a499214)

      ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/05dc88b9-226d-4724-931e-b2464efd7349)

    **Fine-tuning methods**

       - Instruction fine-tuning: It's about training the machine learning model using examples that demonstrate how the model should respond to the query. The dataset you use for fine-tuning large language models has to serve the purpose of your instruction. 

             ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/4cd9d6f7-9808-4463-a912-32a122f11a64)
               
       -  Full fine-tuning: Instruction fine-tuning, where all of the model's weights are updated, is known as full fine-tuning
       -  Parameter-efficient fine-tuning:  PEFT methods only update a small set of parameters

   **Other types of fine-tuning**

      - Transfer learning:  Transfer learning is about taking the model that had learned on general-purpose, massive datasets and training it on distinct, task-specific data. This dataset may include labeled examples related to that domain. Transfer learning is used when there is not enough data or a lack of time to train data; the main advantage of it is that it offers a higher learning rate and accuracy after training. You can take existing LLMs that are pre-trained on vast amounts of data, like GPT ¬æ and BERT, and customize them for your own use case.
      - Task-specific fine-tuning: Task-specific fine-tuning is a method where the pre-trained model is fine-tuned on a specific task or domain using a dataset designed for that domain. This method requires more data and time than transfer learning but can result in higher performance on the specific task.
      - Multi-task learning: Multi-task fine-tuning is an extension of single-task fine-tuning, where the training dataset consists of example inputs and outputs for multiple tasks.
      - Sequential fine-tuning: Sequential fine-tuning is about sequentially adapting a pre-trained model on several related tasks. After the initial transfer to a general domain, the LLM might be fine-tuned on a more specific subset.
      


 * Benefits of Fine Tuning

    ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/46df27cd-0902-443d-b4eb-aea9831ed2dc)

   https://www.superannotate.com/blog/llm-fine-tuning?source=post_page-----fb60abdeba07--------------------------------

 
 

 * RAG Vs Fine-Tuning: How to Optimize LLM Performance
      
        https://www.e2enetworks.com/blog/rag-vs-fine-tuning-how-to-optimize-llm-performance#:~:text=Trade%2Doffs%3A%20Fine%2Dtuning%20may%20provide%20more%20control%20over,reliability%20of%20the%20knowledge%20base.



 * Trade-Offs
  
      The decision to employ fine-tuning or RAG depends on the specific goals of a task and the nature of the knowledge required. Here are some considerations and trade-offs:
      
      Fine-tuning Considerations: Fine-tuning is suitable for tasks where specific, task-oriented improvements are needed. It is effective for refining a model's performance in a particular domain. However, fine-tuning may exhibit instability and might not be the optimal choice for addressing broad knowledge deficits.
      RAG Considerations: RAG excels in knowledge-intensive tasks where external information is valuable which is provided by feeding data to the knowledge base. It can address both knowledge deficits and factual errors by incorporating diverse knowledge from external sources. RAG's effectiveness relies on the quality and coverage of the knowledge base.
      Trade-offs: Fine-tuning may provide more control over specific task-related improvements, but it might struggle with broader knowledge adaptation. RAG, while powerful in leveraging external knowledge, depends on the availability and reliability of the knowledge base.

* H2O LLM DataStudio: Streamlining Data Curation and Data Preparation for LLMs related tasks
  https://h2o.ai/blog/2023/streamlining-data-preparation-for-fine-tuning-of-large-language-models/
* H2O LLM DataStudio Part II: Convert Documents to QA Pairs for fine tuning of LLMs
  https://h2o.ai/blog/2023/h2o-llm-datastudio-part-ii-convert-documents-to-qa-pairs-for-fine-tuning-of-llms/



# RAG


  * What is Retrieval-Augmented Generation (RAG)?, https://www.youtube.com/watch?v=T-D1OfcDW1M&t=265s&ab_channel=IBMTechnology


  * Community Paper Reading: RAG vs Fine-tuning, https://www.youtube.com/watch?v=EbEPHOABgSY&ab_channel=ArizeAI

  * RAG-VectorDB-Embedings-LlamaIndex-Langchain,   https://github.com/lucifertrj/Awesome-RAG
    
  * Better Retrieval Augmented Generation (RAG) with LangChain Parent-Child Retriever, https://www.youtube.com/watch?v=wSi0fxkH6e0
    
  * User-Selected metadata in RAG Applications with Qdrant, https://www.youtube.com/watch?v=qcn7YAJfCeE&ab_channel=LearnDatawithMark
  
  * Ollama Python Library Released! How to implement Ollama RAG? https://www.youtube.com/watch?v=4HfSfFvLn9Q&ab_channel=MervinPraison
  
  * Building a Multimodal RAG App for Medical Applications, https://www.youtube.com/watch?v=fbbFrCfaF0w&ab_channel=AIAnytime
  
  * Track and Monitor RAG Pipelines using Weights & Biases (wandb), https://www.youtube.com/watch?v=8-exaASey6o&ab_channel=AIAnytime
 
  * Unlocking RAG Potential with LLMWare's CPU-Friendly Smaller Models, https://www.youtube.com/watch?v=qXEUqhqjHdg&ab_channel=AIAnytime
 
  * RAG Implementation using Zephyr 7B Beta LLM: Is this the best 7B LLM? https://www.youtube.com/watch?v=btuN-rrPhsM&ab_channel=AIAnytime
 
  * Better RAG with Merger Retriever (LOTR) and Re-ranking Retriever (Long Context Reorder), https://www.youtube.com/watch?v=uYZftCq2efg&ab_channel=AIAnytime
 
  * Bert Score for Contextual Similarity for RAG Evaluation, https://youtube.com/watch?v=7AVjk2k8Mbs&ab_channel=AIAnytime
 
  * Testing Framework Giskard for LLM and RAG Evaluation (Bias, Hallucination, and More), https://www.youtube.com/watch?v=KeY6qPAvyq0&ab_channel=AIAnytime
 
  * Pinecone + LlamaIndex on Retrieval Augmented Generation (RAG) Systems, https://www.youtube.com/watch?v=FgLf5HjxI8w&ab_channel=ArizeAI
    
  * Optimizing RAG With LLMS: Exploring Chunking Techniques and Reranking for Enhanced Results, https://youtube.com/watch?v=QpRTdZDR4tE&ab_channel=ArizeAI

  * Check Hallucination of LLMs and RAGs using Open Source Evaluation Model by Vectara, https://www.youtube.com/watch?v=O-VYDADgc68&ab_channel=AIAnytime
 
  * Learn to Evaluate LLMs and RAG Approaches, https://www.youtube.com/watch?v=97ftVtITKfo&ab_channel=AIAnytime
 
  * Evaluating Biases in LLMs using WEAT and Demographic Diversity Analysis, https://www.youtube.com/watch?v=eTenkUPsjko&ab_channel=AIAnytime
 
  * RAG with LlamaIndex - Qdrant and Azure OpenAI in 9 minutes, https://www.youtube.com/watch?v=h4F09fWhyhg&ab_channel=AmbarishGangulyAcademy

    https://github.com/ambarishg/llama-index

  * LangChain RAG featuring Shopify's Madhav Thaker, https://www.youtube.com/watch?v=IUEny5cbys8&ab_channel=ArizeAI

    https://shopify.engineering/topics/data-science-engineering

  * LLM Search & Retrieval Systems with Arize and LlamaIndex: Powering LLMs on Your Proprietary Data, https://www.youtube.com/watch?v=hbQYDpJayFw&ab_channel=ArizeAI
 
  * Building A RAG System With OpenAI Latest Embeddings, https://www.youtube.com/watch?v=OvvgaR1S4Xc&ab_channel=RichmondAlake

  * Transform RAG and Search with Azure AI Document Intelligence, https://www.youtube.com/watch?v=SOBdR-xxE04&ab_channel=SethJuarez

  * Best retrieval strategies for Generative AI applications: Semantic Search Benchmarking, https://www.youtube.com/watch?v=BvnOln6YZ_8&ab_channel=SethJuarez
    
  * Building corrective RAG from scratch with open source, local LLMs, https://youtu.be/E2shqsYwxck?si=LEeA5KXOQ6idzDd2

  * RAG from scratch, https://youtube.com/playlist?list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x&si=BtJ6KCTMfqBzIGya
 
  * Build with LangChain, https://youtube.com/playlist?list=PLfaIDFEXuae06tclDATrMYY0idsTdLg9v&si=0ypsn2axHsDSMs6b
    
  * LangGraph python, https://youtube.com/playlist?list=PLfaIDFEXuae16n2TWUkKq5PgJ0w6Pkwtg&si=haMafIbDjtLZ9hFU
    
  * CrewAI RAG: How I Created AI Assistants to Run My News Agency, https://www.youtube.com/watch?v=77xSbC-9yn4&ab_channel=MervinPraison

  * Advanced RAG on HuggingFace documentation using langchain, https://huggingface.co/learn/cookbook/advanced_rag

      https://github.com/huggingface/cookbook/tree/main/notebooks/en

      ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/a3b7c78b-c349-481c-a7c7-94191c00ea19)

    
  * RAG Evaluation Using Synthetic data and LLM-As-A-Judge, https://github.com/huggingface/cookbook/tree/main/notebooks/en
    
  * Getting started with RAG in DSPy!,
  https://youtu.be/CEuUG4Umfxs?si=Dz_S5uOXSlo3yiIN

  * RAG Time! Evaluate RAG with LLM Evals and Benchmarking

     ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/1d20253d-7f6f-41d2-86cd-b68bc9e46233)
    
    https://www.youtube.com/watch?v=LrMguHcbpO8&ab_channel=ArizeAI

  * Gemma with transformers: how to teach structured English quotes to LLM
    https://youtu.be/qeJgBkPLCxo?si=YzFFkJop1ptC_YBM

  * Unlock AI Agents, Function Calls and Multi-Step RAG with LLMWare
    https://www.youtube.com/watch?v=cQfdaTcmBpY&ab_channel=llmware











    

  







# Prompt

  * Token Cost Reduction through LLMLingua's Prompt Compression, https://www.youtube.com/watch?v=xLNL6hSCPhc&ab_channel=AIAnytime
 
  * Prompting Guide, https://www.promptingguide.ai/research/rag

  * Prompt Engineering, RAG, and Fine-tuning: Benefits and When to Use, https://www.youtube.com/watch?v=YVWxbHJakgg&ab_channel=EntryPointAI
 
  * Text to Speech Tortoise versus Openvoice Comparison | How to Clone Any Voice for FREE !!, https://www.youtube.com/watch?v=E9jWEmUSxyo&ab_channel=SkillCurb
 
  * ChatGPT Vision API End to End Project with Zapier and MindStudio, https://www.youtube.com/watch?v=4UsQxuhxB7c&ab_channel=SkillCurb

    
  * Vibe-Based Prompt Engineering with PromptLayer's Jared Zoneraich, https://www.youtube.com/watch?v=SEgwj6SVWyQ&ab_channel=ArizeAI
    
  * Prompt Templates, Functions and Prompt Window Management, https://www.youtube.com/watch?v=YaYaZu6NbS0&ab_channel=ArizeAI
  
  * Chat with documents with Chainlit, Langchain, Ollama & Mistral, https://youtu.be/2IL0Sd3neWc?si=eXSH7WZa_bczTfTv
    
  * How I created AI Research Assistantand it costs 0$ to run, Ollama + qdrant + Gptforall + langchain, https://youtu.be/f1ihg20fQiU?si=VjaYv9yr9g-Ujvdk
    




# Vector Database and Embeddings 


* **LanceDB**, a free, open-source, serverless vectorDB that requires no setup.   It integrates into python data ecosystem so you can simply start using these in your existing data pipelines in pandas, arrow, pydantic etc. LanceDB has native Typescript SDK using which you can run vector search in serverless functions!
  
   ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/fb5edda5-8ac4-4e28-9c59-3f220604f444)
  
  https://github.com/lancedb/vectordb-recipes/tree/main

* **Fastmbed**, FastEmbed is a lightweight, fast, Python library built for embedding generation. We support popular text models. Please open a Github issue if you want us to add a new model.

   https://www.youtube.com/watch?v=1mMLVQE11Io&ab_channel=LearnDatawithMark

  https://github.com/qdrant/fastembed

  https://qdrant.github.io/fastembed/

  https://simonwillison.net/2023/Oct/23/embeddings/
  
  
 * Embedding multimodal data for similarity search using ü§ó transformers, ü§ó datasets and FAISS

   https://github.com/huggingface/cookbook/tree/main/notebooks/en


 * Introduction to Matryoshka Embedding Models

   https://huggingface.co/blog/matryoshka


* Ollama 0.1.26 Makes Embedding 100x Better**
  
  https://www.youtube.com/watch?v=Ml179HQoy9o&ab_channel=MattWilliams
  
  nomic-embed-text works very faster than llama2 as of now. 

  https://huggingface.co/nomic-ai/nomic-embed-text-v1


* From HuggingFace dataset to Qdrant vector database in 12 minutes flat

  https://www.gptechblog.com/from-huggingface-dataset-to-qdrant-vector-database-in-12-minutes-flat/


* Transformers and Quadrant: Revolutionizing Data Integration for NLP Tasks

  https://huggingface.co/blog/Andyrasika/qdrant-transformers

* Ollama Embedding: How to Feed Data to AI for Better Response?

   Model
  
   ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/e5581945-1d68-492f-b17b-34854d8bb927)

   Web
  
   ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/c40a56fe-588d-43fe-a2a1-23a1d7f9f88a)



   https://www.youtube.com/watch?v=jENqvjpkwmw&t=17s&ab_channel=MervinPraison




    




# Essentials on LoRA, Quantization, PEFT and their Variants

**LoRA**

   * What is LoRA?
      
       Edward Hu, https://edwardjhu.com/
      
       https://lightning.ai/lightning-ai/studios/code-lora-from-scratch
      
   * LoRA training scripts of the world, unite!
      
       https://huggingface.co/blog/sdxl_lora_advanced_script
     
   * Parameter-Efficient LLM Finetuning With Low-Rank Adaptation (LoRA)
      
      https://lightning.ai/pages/community/tutorial/lora-llm/
      
   * Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments
      
        https://lightning.ai/pages/community/lora-insights/
      
   * Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)
      
         https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms
      
         Hackable implementation of state-of-the-art open-source LLMs based on nanoGPT. Supports flash attention, 4-bit and 8-bit quantization, LoRA and LLaMA-Adapter fine-tuning, pre-training. Apache 2.0-licensed.
        
         https://github.com/Lightning-AI/lit-gpt
        
      
   * Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA
      
        https://huggingface.co/blog/4bit-transformers-bitsandbytes


   * Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation **(DoRA)** from Scratch
      
        https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch
      
        https://github.com/rasbt/dora-from-scratch
      
            
   * Rank-Stabilized LoRA: Unlocking the Potential of LoRA Fine-Tuning
      
        https://huggingface.co/blog/damjan-k/rslora
         
   * Finetuning LLMs with LoRA and QLoRA: Insights from Hundreds of Experiments
      
        https://lightning.ai/pages/community/lora-insights/
        
      
   * A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes
      
        https://huggingface.co/blog/hf-bitsandbytes-integration
      
   * SDXL in 4 steps with Latent Consistency LoRAs
      
        https://huggingface.co/blog/lcm_lora




**Quantization**

         The Two Types of LLM Quantization: PTQ and QAT
         
         While there are several quantization techniques, the most notable of which we detail later in this guide, generally speaking, LLM quantization falls into two categories:
         
         Post-Training Quantization (PTQ): this refers to techniques that quantize an LLM after it has already been trained. PTQ is easier to implement than QAT, as it requires less training data and is faster. However, it can also result in reduced model accuracy from lost precision in the value of the weights. 
         
         Quantization-Aware Training (QAT): this refers to methods of fine-tuning on data with quantization in mind. In contrast to PTQ techniques, QAT integrates the weight conversion process, i.e., calibration, range estimation, clipping, rounding, etc., during the training stage. This often results in superior model performance, but is more computationally demanding. 


* Quantization

  https://huggingface.co/docs/optimum/concept_guides/quantization

* A Guide to Quantization in LLMs

 https://symbl.ai/developers/blog/a-guide-to-quantization-in-llms/

* Quantization in LLMs: Why Does It Matter?

  https://blog.dataiku.com/quantization-in-llms-why-does-it-matter
  
* What are Quantized LLMs?
  
  https://www.tensorops.ai/post/what-are-quantized-llms#:~:text=LLM%20Quantization%20is%20enabled%20thanks,allowing%20it%20to%20be%20run

* The LLM Revolution: Boosting Computing Capacity with Quantization Methods

  https://blog.gopenai.com/the-llm-revolution-boosting-computing-capacity-with-quantization-methods-b8666cdb4b6a
    

* Which Quantization Method is Right for You? (GPTQ vs. GGUF vs. AWQ)
  https://www.maartengrootendorst.com/blog/quantization/

* Quantization and LLMs - Condensing Models to Manageable Sizes
  https://www.exxactcorp.com/blog/deep-learning/what-is-quantization-and-llms
  
* Best LLM quantization (accuracy and speed)
  
  https://scifilogic.com/best-llm-quantization-accuracy-and-speed/
  
* Serving Quantized LLMs on NVIDIA H100 Tensor Core GPUs

  https://www.databricks.com/blog/serving-quantized-llms-nvidia-h100-tensor-core-gpus

* New Tutorial on LLM Quantization w/ QLoRA, GPTQ and Llamacpp, LLama 2

  https://www.youtube.com/watch?v=YEVyupJxt1Q
  
*  How to make your LLMs lighter with GPTQ quantization

  https://bdtechtalks.com/2023/11/08/llm-quantization-gptq/
  
* Model Quantization with ü§ó Hugging Face Transformers and Bitsandbytes Integration

  https://medium.com/@rakeshrajpurohit/model-quantization-with-hugging-face-transformers-and-bitsandbytes-integration-b4c9983e8996


* How to Quantize an LLM with GGUF or AWQ

  https://www.youtube.com/watch?v=XM8pllpBVA0
  
* Effective Post-Training Quantization for Large Language Models

  https://medium.com/intel-analytics-software/effective-post-training-quantization-for-large-language-models-with-enhanced-smoothquant-approach-93e9d104fb98


* Overview of natively supported quantization schemes in ü§ó Transformers

   https://huggingface.co/blog/overview-quantization-transformers


* How to quantization an LLM with GGUF or AWQ
  
  https://youtu.be/XM8pllpBVA0?si=v_jLj78pCnOXIv2i

  https://tinyurl.com/2s58xnam
  
* Making LLMs lighter with AutoGPTQ and transformers
  
  GPTQ blogpost ‚Äì gives an overview on what is the GPTQ quantization method and how to use it.

  https://huggingface.co/blog/gptq-integration
  
  https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing

  
* bistandbytes 4-bit quantization blogpost - This blogpost introduces 4-bit quantization and QLoRa, an efficient finetuning approach

  This blogpost introduces 4-bit quantization and QLoRa, an efficient finetuning approach.

  https://huggingface.co/blog/4bit-transformers-bitsandbytes


* A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes

  bistandbytes 8-bit quantization blogpost - This blogpost explains how 8-bit quantization works with bitsandbytes.

  https://huggingface.co/blog/hf-bitsandbytes-integration

  Basic usage Google Colab notebook for bitsandbytes - This notebook shows how to use 4-bit models in inference with all their variants, and how to run GPT-neo-X (a 20B parameter model) on a free Google Colab instance.

  https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing

* Comparing the Performance of LLMs: A Deep Dive into Roberta, Llama 2, and Mistral for Disaster Tweets Analysis with Lora
    https://huggingface.co/blog/Lora-for-sequence-classification-with-Roberta-Llama-Mistral


* Introduction to Quantization cooked in ü§ó with üíóüßë‚Äçüç≥

  Merve's blogpost on quantization - This blogpost provides a gentle introduction to quantization and the quantization methods supported natively in transformers.

  https://huggingface.co/blog/merve/quantization


* Democratizing LLMs: 4-bit Quantization for Optimal LLM Inference

  https://towardsdatascience.com/democratizing-llms-4-bit-quantization-for-optimal-llm-inference-be30cf4e0e34




**PEFT**

 * ü§ó PEFT welcomes new merging methods
        
         https://huggingface.co/blog/peft_merging






# LLM Apps

  * Visual Question Answering with IDEFICS 9B Multimodal LLM, https://www.youtube.com/watch?v=hyP1ekLKtiI&ab_channel=AIAnytime
  
  * Outfit Anyone: A Diffusion Project for Virtual Try On, https://www.youtube.com/watch?v=V21GfgSFuGk&ab_channel=AIAnytime
    
  * Oncology RAG App - Powered by Meditron 7B Medical LLM, https://www.youtube.com/watch?v=kvbjB-q5Dss&ab_channel=AIAnytime
 
  * Investment Banker RAG Chatbot using Intel's Neural Chat LLM, https://www.youtube.com/watch?v=d9wCHH3iknM&ab_channel=AIAnytime
 
  * Deploy RAG App built using Create Llama on Vercel: Free and Easy Method, https://www.youtube.com/watch?v=euYa4iesOm8&ab_channel=AIAnytime
    
  * Create a LlamaIndex App with Create Llama: No Code tool for RAG, https://www.youtube.com/watch?v=JkGU3d8IM1c&ab_channel=AIAnytime

  * AutoLLM: Ship RAG based LLM Apps and API in Seconds, https://www.youtube.com/watch?v=iTGbwD-sSxM&ab_channel=AIAnytime

  * Query Your CSV using LIDA: Automatic Generation of Visualizations with LLMs, https://www.youtube.com/watch?v=U9K1Cu45nMQ&ab_channel=AIAnytime
 
  * Chat with Data App: RAG using Mistral 7B, Haystack, and Chainlit, https://www.youtube.com/watch?v=01_2-Dy57ys&ab_channel=AIAnytime

  * Question Answer Generator App using Mistral LLM, Langchain, and FastAPI, https://www.youtube.com/watch?v=Hcqmhhx30Pg&ab_channel=AIAnytime
 
  * RAG Implementation using Mistral 7B, Haystack, Weaviate, and FastAPI, https://www.youtube.com/watch?v=C5mqILmVUEo&ab_channel=AIAnytime

  * Let's Build an AI News Anchor Generator App using Generative AI, https://www.youtube.com/watch?v=cddahTnCo10&ab_channel=AIAnytime

  * Screenshot to Code Generation: 10x Faster Frontend/UI Development, https://www.youtube.com/watch?v=52Xq6AaRnT4&ab_channel=AIAnytime

  * ComfyUI GUI for Image and Video Generation: Google Colab Setup, https://www.youtube.com/watch?v=PYEnK_iQeZU&ab_channel=AIAnytime

  * Build Generative AI Agents using Dialogflow CX and Vertex AI on GCP, https://www.youtube.com/watch?v=cDY8lm6vg7w&ab_channel=AIAnytime

  * Question Answer Generator App using Mistral LLM, Langchain, and FastAPI, https://www.youtube.com/watch?v=Hcqmhhx30Pg&ab_channel=AIAnytime

  * Build a Containerized Transcription API using Whisper Model and FastAPI, https://www.youtube.com/watch?v=NU406wZz1eU&ab_channel=AIAnytime

  * Build Your RAG-based ChatGPT Web App with Azure: LawGPT Use Case Tutorial, https://www.youtube.com/watch?v=wmfAJWwyaQA&ab_channel=AIAnytime
 
  * Creating a Veterinary Chatbot using Llama 2: Harnessing Gen AI for Pet Care, https://www.youtube.com/watch?v=Iyzvka711pc&ab_channel=AIAnytime

  * Build Your API for Llama 2 on AWS: Lambda Function and API Gateway, https://www.youtube.com/watch?v=Nlo7WclRBXc&t=512s&pp=ygUGb2xsYW1h

  * Deploy Llama 2 for your Entire Organisation, https://www.youtube.com/watch?v=Ror2xOOA-VE&ab_channel=TrelisResearch

  * Install and Run Mistral 7B on AWS, https://www.youtube.com/watch?v=aSh66tG1B5o&pp=ygUNb2xsYW1hIG9uIEFXUw%3D%3D

  * Deploy Llama 2 on AWS SageMaker using DLC (Deep Learning Containers), https://www.youtube.com/watch?v=rQq1m2aJ_fk&ab_channel=AIAnytime
 
  * Enterprise Chat App using Azure Cognitive Search and Azure OpenAI: End-to-End Tutorial, https://www.youtube.com/watch?v=hkSnPhhjm1Y&ab_channel=AIAnytime

  * Containerizing LLM-Powered Apps: Part 1 of the Chatbot Deployment, https://www.youtube.com/watch?v=7CeAJ0EbzDA&ab_channel=AIAnytime

  * Deploy LLM Powered Apps on Azure App Service: Part 2 of the Chatbot Deployment, https://www.youtube.com/watch?v=vYIlhgVHAls&ab_channel=AIAnytime

  * Serve a Custom LLM for Over 100 Customers, https://www.youtube.com/watch?v=1TU9ZrZhqw0&ab_channel=TrelisResearch

  * Long Context Summarization, https://www.youtube.com/watch?v=I83TH4x9keo&ab_channel=TrelisResearch

  * Function Calling Datasets, Training and Inference, https://www.youtube.com/watch?v=hHn_cV5WUDI&ab_channel=TrelisResearch
 
  * How to Build an OpenAI LLM on a Private Network with AWS, https://www.youtube.com/watch?v=6LGGQERxrQo&ab_channel=SingleStore
 
  * Amazon Bedrock: Generative AI on AWS without the Headaches, https://www.youtube.com/watch?v=Yj_7FuFgPyI

  * FULLY LOCAL Mistral AI PDF Processing Hands-on Tutorial, https://www.youtube.com/watch?v=wZDVgy_14PE&pp=ygUNb2xsYW1hIG9uIEFXUw%3D%3D

  * PrivateGPT 2.0 - FULLY LOCAL Chat With Docs (PDF, TXT, HTML, PPTX, DOCX, and more), https://www.youtube.com/watch?v=XFiof0V3nhA&ab_channel=MatthewBerman
 
  * AutoLLM: Create RAG Based LLM Web Apps in SECONDS!, https://www.youtube.com/watch?v=kPaiZe_qD34&ab_channel=WorldofAI
 
  * Use OpenChat and LM Studio with LLMWare, https://www.youtube.com/watch?v=h2FDjUyvsKE&ab_channel=llmware

  * Compare Embedding Models for Side by Side Queries Using Postgres with LLMWare, https://www.youtube.com/watch?v=Bncvggy6m5Q&ab_channel=llmware
    
  * AutoGen Studio with 100% Local LLMs (LM Studio), https://www.youtube.com/watch?v=ob45YmYD2KI&ab_channel=PromptEngineering
  
  * AutoGen Studio UI 2.0: Easiest Way to Create Custom Agents, https://www.youtube.com/watch?v=KIvl-VY8H0Y&ab_channel=PromptEngineering
    
  * Your LLM Powered Financial Analyst, https://www.youtube.com/watch?v=JeruKKuMxCg&ab_channel=PromptEngineering
 
  * Self-reflective RAG with LangGraph: Self-RAG and CRAG, https://www.youtube.com/watch?v=pbAd8O1Lvm4&ab_channel=LangChain

    https://github.com/langchain-ai/langgraph/tree/main/examples/rag


  * AutoGen + Panel Ep.3 - Web UI for Multi-agent with Document Retrieval

    https://www.youtube.com/watch?v=98Ri4VVBP_8&t=431s

     https://github.com/yeyu2/Youtube_demos


  * How to Create a Web UI for AutoGen by Using Panel

    https://www.youtube.com/watch?v=mFmPDyLlj1E

    https://github.com/yeyu2/Youtube_demos


  * Create Full Function UI for AutoGen Powered by Panel (Human Input Enabled)

    https://www.youtube.com/watch?v=9lSaRP9GLCY


  * AutoGen + Function Calling + Open Source LLMs, Here is How

    https://www.youtube.com/watch?v=UIBerUGqHjc&ab_channel=YeyuLab

  * Use Open Source LLMs in AutoGen powered by Fireworks AI, without GPU/CPU

    https://www.youtube.com/watch?v=HN96PTdiseo&ab_channel=YeyuLab


  * Hands on with LangGraph Agent Workflows: Build a LangChain Coding Agent with Custom Tools
 
    https://www.youtube.com/watch?v=oMRJ--GJCKQ&ab_channel=DeployingAI

  * Development with Large Language Models Tutorial ‚Äì OpenAI, Langchain, Agents, Chroma

    https://www.youtube.com/watch?v=xZDB1naRUlk

  * Make an offline GPT voice assistant in Python
  
    https://youtu.be/w5unVTO7mLQ?si=LLictvhoG4hm2JJy

  * Build and Run a Medical Chatbot using Llama 2 on CPU Machine: All Open Source, https://www.youtube.com/watch?v=kXuHxI5ZcG0&ab_channel=AIAnytime
    
  * Chat With Websites Using ChainLit / Streamlit, LangChain, Ollama & Mistral üß†, https://www.youtube.com/watch?v=FZrkm0vaYYQ&ab_channel=DataScienceBasics

    https://github.com/sudarshan-koirala/chat-with-website

  * LocalGPT API: Serve Multiple Users At the Same time, https://www.youtube.com/watch?v=z9wDKwgQojM&ab_channel=PromptEngineering
    
  * Deploy and Use any Open Source LLMs using RunPod, https://www.youtube.com/watch?v=nHuHGoLSXb0&ab_channel=AIAnytime

  * CPU-based SLMs for AI Agents and Function Calling by LLMWare, https://www.youtube.com/watch?v=0MOMBJjytkQ&ab_channel=AIAnytime

  * Function Calling using Open Source LLM (Mistral 7B), https://www.youtube.com/watch?v=MQmfSBdIfno&t=337s&ab_channel=AIAnytime
    
  * vector search, RAG, and Azure AI search,

    https://speakerdeck.com/pamelafox/vector-search-and-retrieval-for-generative-ai-app-microsoft-ai-tour-sf

    https://www.youtube.com/live/vuOA13Y_Qzk?si=bT6zY4piPt_yUn_Q

    https://github.com/pamelafox/vector-search-demos

    https://pamelafox.github.io/vectors-comparison

    https://github.com/Azure-Samples/azure-search-openai-demo


  * Manage vector databases and long term memory in flowwise, AI vector tools Review part 1
  
   https://youtu.be/d7nAcshOe4w?si=kArGQ_Ua8pFdvzFy

  * Learn how to use LlamaIndex with LanChainin Flowwise,LlamaIndex vs Langchain part 2,
  
    https://youtu.be/KVOWPeV9s00?si=T9K6edpHIcAr0BBS

  * Create a Web Interface for your LLM in Python

    https://huggingface.co/blog/Alex1337/create-a-web-interface-for-your-llm-in-python

    Turns Data and AI algorithms into production-ready web applications in no time.

    https://github.com/Avaiga/taipy

    https://www.taipy.io/

  * I made AI to auto categorise 10000 comments on Google Sheet with 0$
 
     https://youtu.be/wXiTuNnh2h4?si=P58oj6TLjhqOmtOD


  * Build a medical RAG app using Biomistral, Qdrant and Llama.cpp

    https://youtu.be/A_m3tCqdts4?si=23s00oY8opM8i2PR


  * Steerable AI with Pinecone + Semantic router, https://youtu.be/qjRrMxT20T0?si=hQj7YxUJAj2Y2unV

  * Constitutional AI with Open LLMs

    https://huggingface.co/blog/constitutional_ai

    https://github.com/huggingface/alignment-handbook/tree/main/recipes/constitutional-ai

  * Stop paying for ChatGPT with these two tools | LMStudio x AnythingLLM\

    https://www.youtube.com/watch?v=-Rs8-M-xBFI&ab_channel=TimCarambat

  * Create Chat UI Using ChainLit, LangChain, Ollama & Gemma üß†
    https://www.youtube.com/watch?v=n9AMtXLveMs&t=11s&ab_channel=DataScienceBasics

  * LangSmith For Beginners | Must know LLM Evaluation Platform üî•
    https://www.youtube.com/watch?v=FgG-trkAMwU&ab_channel=DataScienceBasics


 * Create-Llama: deploy LlamaIndex RAG App to Vercel
 
   https://youtu.be/D8PM89Xry7Q?si=N52WpnQn-CsUqHex


 * PhiData: How to Seamlessly Integrate AI into Your Application

   https://www.youtube.com/watch?v=fLGj63fiYfM&ab_channel=MervinPraison

      ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/c03206bd-997c-432c-be61-5452fd0fdd85)

      ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/80c10a2b-66fd-4968-a2a5-a1374b22a057)

 * Boost Gmail Efficiency with AI: Python Tutorial (CrewAI, LangChain, LangGraph)

    https://www.youtube.com/watch?v=o4-4NvrcOvs&ab_channel=AIFORDEVS 

    https://github.com/joaomdmoura/crewAI
    
  * Create Complex Research Analysis with AI Agents using SLIM models on CPU with LLMWare

    https://www.youtube.com/watch?v=y4WvwHqRR60&ab_channel=llmware
    
      ![image](https://github.com/ParthaPRay/LLM-Learning-Sources/assets/1689639/d6bdb92a-21e1-4ca6-9324-83bf63f352ac)

    https://huggingface.co/llmware

    https://github.com/llmware-ai/llmware

    https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/

    

    






# HuggingFace

* Hugging Face Text Generation Inference available for AWS Inferentia2

  https://huggingface.co/blog/text-generation-inference-on-inferentia2

  This tutorial shows how easy it is to deploy a state-of-the-art LLM, such as **Zephyr 7B**, on **AWS Inferentia2** using **Amazon SageMaker**. Zephyr is a 7B _fine-tuned_ version of **mistralai/Mistral-7B-v0.1** that was trained on a mix of publicly available and synthetic datasets using **Direct Preference Optimization (DPO)**, as described in detail in the technical report. The model is released under the **Apache 2.0 license**, ensuring wide accessibility and use.

      Following steps are performed:
      
      1. Setup development environment
      2. Retrieve the TGI Neuronx Image
      3 .Deploy Zephyr 7B to Amazon SageMaker
      4. Run inference and chat with the model


* Pushing Models and Adapters to HuggingFace | Free Notebook, 

  https://www.youtube.com/watch?v=Kd4JL7GnR8Y&ab_channel=TrelisResearch

  https://github.com/TrelisResearch/install-guides/blob/main/Pushing_to_Hub.ipynb

  https://awsdocs-neuron.readthedocs-hosted.com/en/latest/

  https://huggingface.co/docs/optimum-neuron/index


* Deep Dive: Hugging Face models on AWS AI Accelerators

  https://www.youtube.com/watch?v=66JUlAA8nOU&ab_channel=JulienSimon

* A guide to setting up **your own** Hugging Face **leaderboard**: an end-to-end example with Vectara's hallucination leaderboard

  https://huggingface.co/blog/leaderboards-on-the-hub-vectara


* The Hallucinations Leaderboard, an Open Effort to Measure Hallucinations in Large Language Models
  https://huggingface.co/blog/leaderboards-on-the-hub-hallucinations

* Creating open machine learning datasets? Share them on the Hugging Face Hub!
  https://huggingface.co/blog/researcher-dataset-sharing

* Deploy Embedding Models with Hugging Face Inference Endpoints
  https://huggingface.co/blog/inference-endpoints-embeddings

* Bhilding a self-corrective coding assistant from scratch
  https://youtu.be/MvNdgmM7uyc?si=b78VIhFapFo2U8NV
  




# Pipepline

   * ML pipeline with Pandas and Sklearn, https://www.youtube.com/watch?v=Zpy9npXnW00&ab_channel=RicardoCalix







# Pervasive Generative AI

  * Using Ollama to Run Local LLMs on the Raspberry Pi 5, https://www.youtube.com/watch?v=ewXANEIC8pY&ab_channel=IanWootten

  * Private AI Revolution: Setting Up Ollama with WebUI on Raspberry Pi 5!, https://www.youtube.com/watch?v=jJKbYj8mIy8&ab_channel=KevinMcAleer

  * I Ran Advanced LLMs on the Raspberry Pi 5!, https://www.youtube.com/watch?v=Y2ldwg8xsgE&ab_channel=DataSlayer

  * How to Run a ChatGPT-like AI on Your Raspberry Pi, https://www.youtube.com/watch?v=idZctq7WIq4&ab_channel=GaryExplains

  * Local AI Just Got Easy (and Cheap), https://www.youtube.com/watch?v=mdOEaNV8NXw&ab_channel=DataSlayer

      Following boards are needed:
      
      1. Zima Board
      2. Coral USB TPU
      3. Coral PCie TPU
      4. M.2 Adapter
      5. Raspberry Pi 5


  * Power of Generative AI + Common-Sense of Reasoning AI = All-Pervasive Conversational Ux, https://www.youtube.com/watch?v=j1uZ1NpC_4M&ab_channel=Travellingwave

    Paper Link: www.isca-speech.org/archive/pdfs/interspeech_2023/rao23_interspeech.pdf or www.travellingwave.com/TwIS2023.pdf

  * Running SDXL on the Raspberry Pi 5 is now POSSIBLE!, https://www.youtube.com/watch?v=XVS8oiuU6sA&ab_channel=AiFlux

  * World's Easiest GPT-like Voice Assistant
    https://github.com/nickbild/local_llm_assistant?tab=readme-ov-file







# AGI

* **OpenAI-backed "AGI ROBOT" SHOCKED The ENTIRE Industry**, https://www.youtube.com/watch?v=yauNW4C-Tfo&ab_channel=MatthewBerman




# Responsible AI

  https://youtube.com/playlist?list=PL8P_Z6C4GcuVMxhwT9JO_nKuW0QMSJ-cZ&si=vtxnKLMZwB8SGz6y

  https://github.com/aws-samples/aws-machine-learning-university-responsible-ai/



# Cloud GPUs

https://fullstackdeeplearning.com/cloud-gpus/

By Sergey Karayev and Charles Frye. Updated October 30, 2023.

Discussion of this page on Hacker News [https://news.ycombinator.com/item?id=36025099] May 21, 2023.



# General ML, DL


* **How to convert any problem into a machine learning problem**

  https://www.youtube.com/watch?v=-MTW39At8F0&ab_channel=RicardoCalix

* **Intro to Reinforcement Learning through Human Feedbacks (RLHF)**
  
  https://www.youtube.com/watch?v=A8YqZKGRTAM&ab_channel=RicardoCalix

* **A Simple Generative Adversarial Network (GAN) in PyTorch**

  https://www.youtube.com/watch?v=BGtSw0XNthY&ab_channel=RicardoCalix

* **Learn More about ML and AI and Gen AI on** https://www.youtube.com/@ricardocalix188/videos

      * **Full  Stack Deep LEarning Course for Free**
      
         - FSDL 2022 (Online): A fully online course, taught via YouTube, Crowdcast, and Discord.
         - FSDL 2021 (Online): Contemporaneous with the Berkeley course, we taught an online cohort course.
         - FSDL 2021 (Berkeley): Taught as a UC Berkeley undergrad course CS194-080 in Spring 2021
         - FSDL 2020 (UW): Taught as University of Washington Professional Master's Program course CSEP 590C in Spring 2020
         - FSDL 2019 (Online): Materials from the November 2019 bootcamp held on Berkeley campus organized in a nice online format.
         - FSDL 2019 (Bootcamp): Raw materials from the March 2019 bootcamp, held on Berkeley campus.
         - FSDL 2018 (Bootcamp): Our first bootcamp, held on Berkeley campus in August 2018

      *  **Deep Learning Fundamentals (Full Stack Deep Learning - Spring 2021)**
      
         https://www.youtube.com/watch?v=fGxWfEuUu0w&list=PL1T8fO7ArWlcWg04OgNiJy91PywMKT2lv&ab_channel=TheFullStack
      
      * **Full Stack Deep Learning - 2022**
      
        https://www.youtube.com/watch?v=-Iob-FW5jVM&list=PL1T8fO7ArWleMMI8KPJ_5D5XSlovTW_Ur&ab_channel=TheFullStack










  
# Youtube Channels

* **Mervin Praison** https://www.youtube.com/@MervinPraison
* **Prompt Engineering** https://www.youtube.com/@engineerprompt
* **WorldofAI** https://www.youtube.com/@intheworldofai
* **AlejzndroAO Software and AI**, https://youtube.com/@alejandro_ao?si=1TRHMqnIpQGUjJG6
* **Arize AI**  https://www.youtube.com/@arizeai/videos
* **Learn Data With Mark** https://youtube.com/@learndatawithmark?si=Sf7QWUJd6Jn2K5CR
* **SkillCurb** https://www.youtube.com/@skillcurb
* **Venelin Valkov** https://www.youtube.com/@venelin_valkov
* **Ricardo Calix**, https://www.youtube.com/@ricardocalix188
* **Seth Juarez**
https://www.youtube.com/@sethjuarez
* **Nicholas Renotte** https://www.youtube.com/@NicholasRenotte/
* **Trelus Research** https://youtube.com/@TrelisResearch?si=We9ORBTjY3teMpq4
* **Mat Williams** https://youtube.com/@technovangelist?si=UiLCumC6anKxbzB-
* **Connor Shorten** https://youtube.com/@connorshorten6311?si=YA9lHWPqWaAdOtSy
* **Ian Wootten** https://youtube.com/@IanWootten?si=4xbHzdFIIX7n9SMS
* **AI for Devs** https://youtube.com/@ai-for-devs?si=4TrsM8CP7VBO-2a_
* **Jeremy Howard** https://www.youtube.com/@howardjeremyp
* **Leon Explains AI** https://www.youtube.com/@leonsaiagency
* **Skill Leap AI** https://www.youtube.com/@AppOfTheDay
* **Matthew Berman** https://www.youtube.com/@matthew_berman
* **AI Flux**  https://www.youtube.com/@aifluxchannel
* **Julien Simon** https://www.youtube.com/@juliensimonfr
* **AI Jason** https://www.youtube.com/@AIJasonZ
* **Abhishek Thakur** https://www.youtube.com/@abhishekkrthakur
* **Decoder** https://youtube.com/@decoder-sh?si=OtRKUHqzVgSDT8BC
* **Fahd Mirza** https://www.youtube.com/@fahdmirza
* **Gao Dalie** https://www.youtube.com/@GaoDalie97
* **Yeyu Lab** https://www.youtube.com/@yeyulab
* **James Briggs** https://www.youtube.com/@jamesbriggs
* **AI Anytime** https://www.youtube.com/@AIAnytime
* **All About AI** https://www.youtube.com/@AllAboutAI
* **Sam Witteveen** https://www.youtube.com/@samwitteveenai
* **AutoGPT Tutorials** https://www.youtube.com/@AutoGPTTutorial
* **AI Makerspace** https://www.youtube.com/@AI-Makerspace
* **AssemblyAI** https://www.youtube.com/@AssemblyAI
* **Entry Point AI** https://www.youtube.com/@EntryPointAI
* **Steve (Builder.io)** https://www.youtube.com/@Steve8708
* **Andrej Karpathy** https://youtu.be/VMj-3S1tku0?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ
* **AI Engineer** https://www.youtube.com/@aiDotEngineer
* **Kurdiez** https://www.youtube.com/@kurdiez_en
* **Whispering AI** https://www.youtube.com/@WhisperingAI/videos
* **Shaw Talebi** https://www.youtube.com/@ShawhinTalebi
* **Greg Kamradt (Data Indy)** https://www.youtube.com/@DataIndependent
* **Deci Ai** https://youtube.com/@deciai?si=udeFtVlH6uTJYMfo
* **Rob Mulla** https://www.youtube.com/@robmulla
* **Edward Hu**
https://edwardjhu.com/
* **Deploying AI**
https://youtube.com/@deployingai?si=pXZDOETUDdqiB_9I
* **llmware** https://www.youtube.com/@llmware/videos
* **DataScience Basics** https://www.youtube.com/@llmware
https://youtube.com/@datasciencebasics?si=7jtQNnu2ovM0p_ge
* **JakeEh**, https://youtube.com/@jakeeh?si=m1gSOQIkJbhPxJmt

      * **Jeff Heaton** https://youtube.com/@HeatonResearch?si=hfcA9vNxWsk05Uws
      
           www.heatonresearch.com












# Courses

      * **Free Course on** (https://course.fast.ai/) by Jeremy Howard's Fastai
            
      **Practical Deep Learning:** A free course designed for people with some coding experience, who want to learn how to apply deep learning and machine learning to practical problems.

      Book PDF: https://dl.ebooksworld.ir/books/Deep.Learning.for.Coders.with.fastai.and.PyTorch.Howard.Gugger.OReilly.9781492045526.EBooksWorld.ir.pdf
   



* **LLM University**

 LLM University by Cohere
 
 https://docs.cohere.com/docs/llmu



* **LLM Bootcamp**

  LLM Bootcamp - Spring 2023

  https://fullstackdeeplearning.com/llm-bootcamp/

  **The Full Stack** (https://www.youtube.com/@The_Full_Stack/videos)

  **Lectures** https://www.youtube.com/watch?v=twHxmU9OxDU&list=PL1T8fO7ArWleyIqOy37OVXsP4hFXymdOZ&pp=iAQB
  
     - Learn to Spell: Prompt Engineering
     - LLMOps
     - UX for Language User Interfaces
     - Augmented Language Models
     - Launch an LLM App in One Hour
     - LLM Foundations
     - Project Walkthrough: askFSDL
     - What's Next?
     - Invited Talks
     - Reza Shabani: How To Train Your Own LLM
     - Harrison Chase: Agents
     - Fireside Chat with Peter Welinder

  

* **Machind Learning University by AWS**, https://youtube.com/@machinelearninguniversity1942?si=pD5dszE0HTiOclcu

  https://github.com/aws-samples/aws-machine-learning-university-accelerated-nlp

  https://github.com/aws-samples/aws-machine-learning-university-accelerated-cv

  https://github.com/aws-samples/aws-machine-learning-university-accelerated-tab

  https://github.com/aws-samples/aws-machine-learning-university-dte

  https://github.com/aws-samples/aws-machine-learning-university-responsible-ai
  




* **CS50**

This is CS50, Harvard University's introduction to the intellectual enterprises of computer science and the art of programming. Demanding, but definitely doable. Social, but educational. A focused topic, but broadly applicable skills. CS50 is the quintessential Harvard (and Yale!) course.

 https://www.youtube.com/@cs50





* **Edx:** cs50.edx.org





* **FreeCodeCamp** https://www.youtube.com/@freecodecamp





* **Create a Large Language Model from Scratch with Python ‚Äì Tutorial** https://www.youtube.com/watch?v=UU1WVnMk4E8&t=24s
* **Prompt Engineering for Web Devs - ChatGPT and Bard Tutorial** https://youtu.be/ScKCy2udln8
* **Deep Learning for Computer Vision with Python and TensorFlow ‚Äì Complete Course** https://youtu.be/IA3WxTTPXqQ
* **Machine Learning with Python and Scikit-Learn ‚Äì Full Course** https://youtu.be/hDKCxebp88A
* **MLOps Course ‚Äì Build Machine Learning Production Grade Projects**  https://youtu.be/-dJPoLm_gtE
* **code_your_own_AI** https://www.youtube.com/@code4AI
* **The Ethics of AI & Machine Learning - Full Course** https://youtu.be/qpp1G0iEL_c





* **Google**

**Google Cloud Skills Boost** https://www.cloudskillsboost.google/paths/118


